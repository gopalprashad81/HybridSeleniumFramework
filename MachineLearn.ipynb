{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1E7273L3XkwL_ZFIdbEFIYryyicngiznz",
      "authorship_tag": "ABX9TyMdmL6Q4kMFtvRwlRXkHkl2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gopalprashad81/HybridSeleniumFramework/blob/main/MachineLearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLZfaA1bbCo2"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# Core Libraries\n",
        "# ============================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ============================\n",
        "# Scikit-Learn (sklearn)\n",
        "# ============================\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# ============================\n",
        "# LightGBM\n",
        "# ============================\n",
        "import lightgbm as lgb\n",
        "\n",
        "# ============================\n",
        "# Visualization\n",
        "# ============================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ============================\n",
        "# Optional: Suppress Warnings\n",
        "# ============================\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lecture 3\n",
        "# rmse_train=metrics.root_mean_squared_error(y_true=train['y'],y_pred=old.predict(train.drop(columns\n",
        "from sklearn.metrics import median_absolute_error\n",
        "msdae_train = median_absolute_error(y_true=train['y'], y_pred=pls.predict(train.drop(columns=['y'])))\n",
        "msdae_test =median_absolbute_error(x_test, x_pred_test)                                                                                     )))"
      ],
      "metadata": {
        "id": "dKQQsQ7ekS-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.linear_model"
      ],
      "metadata": {
        "id": "rht0wTvbrGkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "huber=  HuberRegressor().fit(X_train,y_train)\n",
        "y_pred_huber  = huber.predict(x_test)\n",
        "# compare fits on test sample\n",
        "rmse_ols =mean_squared_error(y_test,y_pred_ols)\n",
        "rmse_huber = mean_sqaured_error(y_test,y_pred_huber,squared=False)\n",
        "\n",
        "print(\"rmse_ols\", rmse_ols)\n",
        "print(\"rmse_huber\",rmse_huber)"
      ],
      "metadata": {
        "id": "7QVbqyHCrM61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Problem Set 3 â€” Elastic Net Prediction of Monthly Stock Returns\n",
        "# Author: Gopal Prashad\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import dask.dataframe as dd  # Use Dask for out-of-core processing to handle large data\n",
        "from dask import delayed, compute\n",
        "from sklearn.linear_model import ElasticNetCV\n",
        "from sklearn.model_selection import PredefinedSplit\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\n",
        "\n",
        "try:\n",
        "    from memory_profiler import profile\n",
        "except ImportError:\n",
        "    print(\"memory_profiler not installed. Install via 'pip install memory_profiler' for usage tracking.\")\n",
        "\n",
        "\n",
        "    def profile(func):  # Dummy decorator if not installed\n",
        "        return func\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# LOAD AND CLEAN DATA (Hints #2, #6) - Use Dask for memory efficiency\n",
        "# ============================================================\n",
        "data_dir = '/content/Problem3/\n",
        "\n",
        "# Load with Dask (lazy, partitioned for large files)\n",
        "df = dd.read_parquet(data_dir + 'OpenAP_Macro.parquet.gzip', engine='pyarrow')\n",
        "print(\"Loaded dataset lazily with Dask.\")\n",
        "\n",
        "# Standardize column names to lowercase (compute meta)\n",
        "df.columns = [str(c).strip().lower() for c in df.columns]\n",
        "\n",
        "# Print columns and head to identify names (compute small sample)\n",
        "print(\"Dataframe columns after loading:\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "print(\"\\nDataframe head:\")\n",
        "print(df.head())\n",
        "\n",
        "# Convert date to datetime and align to month-end (Hint #2) - Dask supports vectorized ops\n",
        "df['date'] = dd.to_datetime(df['dateym']) + pd.offsets.MonthEnd(0)\n",
        "\n",
        "# Add year column for filtering\n",
        "df['year'] = df['date'].dt.year\n",
        "\n",
        "# Filter for years 2000 and afterwards (lazy filter)\n",
        "df = df[df['year'] >= 2000]\n",
        "print(f\"Filtered to years >= 2000: {df['date'].min().compute()} to {df['date'].max().compute()}\")\n",
        "\n",
        "\n",
        "# Downcast in Dask (meta propagation for dtypes)\n",
        "def downcast_df_partition(df_partition):\n",
        "    float_cols = df_partition.select_dtypes(include=['float64']).columns\n",
        "    df_partition[float_cols] = df_partition[float_cols].astype('float32')\n",
        "    int_cols = df_partition.select_dtypes(include=['int64']).columns\n",
        "    df_partition[int_cols] = df_partition[int_cols].apply(pd.to_numeric, downcast='integer')\n",
        "    return df_partition\n",
        "\n",
        "\n",
        "# Apply downcast per partition (lazy)\n",
        "df = df.map_partitions(downcast_df_partition, meta=df)\n",
        "\n",
        "print(\"Downcasted dtypes for memory efficiency.\")\n",
        "\n",
        "# ============================================================\n",
        "# IDENTIFY KEY COLUMNS AND VERIFY INTEGRITY (Data Hygiene)\n",
        "# ============================================================\n",
        "\n",
        "# Identify firm identifier (permno), date, and return column dynamically\n",
        "permno_col = next((c for c in ['permno', 'id'] if c in df.columns), None)\n",
        "assert permno_col is not None, \"No firm identifier (permno/id) found.\"\n",
        "\n",
        "ret_col = next((c for c in ['retadj', 'return', 'ret_excess'] if c in df.columns), None)\n",
        "assert ret_col is not None, \" No return column found.\"\n",
        "\n",
        "# Verify no missing keys (compute to check)\n",
        "assert (~df[permno_col].isna()).all().compute() and (~df['date'].isna()).all().compute(), \"Missing firm IDs or dates.\"\n",
        "\n",
        "# Check for duplicate keys (group and count, compute max)\n",
        "duplicates = df.groupby([permno_col, 'date']).size().max().compute()\n",
        "assert duplicates == 1, f\"Duplicates found: max group size {duplicates}\"\n",
        "\n",
        "print(\" Primary key (permno, date) uniqueness confirmed.\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# FEATURE IDENTIFICATION AND PREPROCESSING (Hints #3, #4)\n",
        "# ============================================================\n",
        "# Exclude non-features\n",
        "exclude_cols = {permno_col, 'date', 'year', ret_col}\n",
        "macro_vars = ['macro_dp', 'macro_ep', 'macro_bm', 'macro_ntis', 'macro_tbl', 'macro_tms', 'macro_dfy', 'macro_svar']  # Goyal macros without slashes\n",
        "# Assert macro vars exist\n",
        "missing_macros = [v for v in macro_vars if v not in df.columns]\n",
        "assert not missing_macros, f\"Missing macro variables: {missing_macros}. Check column names.\"\n",
        "\n",
        "firm_vars = [col for col in df.columns if col not in exclude_cols and col not in macro_vars and 'float' in str(df[col].dtype) or 'int' in str(df[col].dtype)]\n",
        "\n",
        "# Fill missing firm vars with period median (Hint #4, Gu et al. 2020) - Dask groupby.transform\n",
        "for col in firm_vars:\n",
        "    df[col] = df.groupby('date')[col].transform(lambda x: x.fillna(x.median()), meta=(col, 'f8'))\n",
        "\n",
        "# Fill any remaining NaNs in firm_vars with 0 (neutral value after ranking)\n",
        "for col in firm_vars:\n",
        "    df[col] = df[col].fillna(0)\n",
        "\n",
        "# Fill missing macro vars with overall mean (compute mean first)\n",
        "for col in macro_vars:\n",
        "    mean_val = df[col].mean().compute()\n",
        "    df[col] = df[col].fillna(mean_val)\n",
        "\n",
        "# Identify binary firm vars (compute unique per col - expensive, so sample or compute on subset if large)\n",
        "binary_vars = []\n",
        "non_binary_firm = []\n",
        "for col in firm_vars:\n",
        "    uniques = df[col].dropna().unique().compute()\n",
        "    if set(uniques) <= {0, 1}:\n",
        "        binary_vars.append(col)\n",
        "    else:\n",
        "        non_binary_firm.append(col)\n",
        "\n",
        "# Preprocess: rank non-binary to [-1,1], map binary to {-1,1} (Hint #3) - Custom partition func for ranking (Dask doesn't support rank(pct=True) directly)\n",
        "def preprocess_partition(partition):\n",
        "    \"\"\"Partition-wise preprocessing: Rank continuous features to [-1,1] interval for outlier robustness and scale invariance.\"\"\"\n",
        "    partition = partition.reset_index(drop=True)  # For local ranking\n",
        "    for col in non_binary_firm:\n",
        "        ranks = partition[col].rank(pct=True)\n",
        "        partition[col] = 2 * ranks - 1\n",
        "    for col in binary_vars:\n",
        "        partition[col] = 2 * partition[col] - 1\n",
        "    return partition\n",
        "\n",
        "# Apply per partition (approximates global per-date, but for exact per-date ranking, compute groupby - may need .compute() if too large)\n",
        "# Note: For exact cross-sectional ranking, better to groupby 'date' but Dask groupby.apply is shuffle-heavy; approx with partitions if dates sorted\n",
        "df = df.map_partitions(preprocess_partition, meta=df)\n",
        "\n",
        "# Assert no remaining NaNs in predictors (compute check)\n",
        "predictors = firm_vars + macro_vars  # No interactions between macro and firm vars\n",
        "assert not df[predictors].isna().any().any().compute(), \"NaNs still present in predictors after imputation.\"\n",
        "\n",
        "print(f\" Preprocessed {len(firm_vars)} firm vars and {len(macro_vars)} macro vars.\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# ROLLING WINDOW SETUP AND MODEL FITTING (Hints #5, #7, #10, #11)\n",
        "# ============================================================\n",
        "start_year = 2000\n",
        "initial_train_years = 6\n",
        "val_years = 3\n",
        "\n",
        "# Get unique years for annual retraining (compute)\n",
        "years = sorted(df['year'].unique().compute())\n",
        "test_years = [y for y in years if y >= start_year + initial_train_years + val_years]\n",
        "\n",
        "\n",
        "@profile  # Memory profile the function if profiler installed\n",
        "def process_year(test_year, df, predictors, ret_col):\n",
        "    \"\"\"Process a single test year: Train model annually, predict for all months in the year using monthly-updated predictors.\"\"\"\n",
        "    train_end = test_year - val_years - 1\n",
        "    val_start = train_end + 1\n",
        "    val_end = test_year - 1\n",
        "\n",
        "    # Create separate training and validation DataFrames (lazy)\n",
        "    train_df = df[(df['year'] >= start_year) & (df['year'] <= train_end)]  # Training set\n",
        "    val_df = df[(df['year'] >= val_start) & (df['year'] <= val_end)]  # Validation set\n",
        "    test_df = df[df['year'] == test_year]  # Test set (all months in year)\n",
        "\n",
        "    # Compute subsets to memory for sklearn (only load needed data)\n",
        "    train_df = train_df.compute()\n",
        "    val_df = val_df.compute()\n",
        "    test_df = test_df.compute()\n",
        "\n",
        "    # Combine in memory for fitting\n",
        "    train_val_df = pd.concat([train_df, val_df])\n",
        "    test_fold = np.concatenate([-1 * np.ones(len(train_df)), np.zeros(len(val_df))])  # -1: train, 0: val\n",
        "\n",
        "    # Create PredefinedSplit CV object for validation separation\n",
        "    ps = PredefinedSplit(test_fold)\n",
        "\n",
        "    X_tv = train_val_df[predictors]\n",
        "    y_tv = train_val_df[ret_col]\n",
        "\n",
        "    # ElasticNetCV: Tune alpha and l1_ratio with predefined CV split, parallelize with n_jobs\n",
        "    enet = ElasticNetCV(alphas=[1, 10], l1_ratio=[0.5, 0.9], cv=ps, n_jobs=-1, random_state=42)\n",
        "    enet.fit(X_tv, y_tv)  # Fit model with training/validation separation\n",
        "\n",
        "    # Predict for all months in test year using monthly-updated predictors\n",
        "    X_test = test_df[predictors]\n",
        "    y_pred = enet.predict(X_test)\n",
        "\n",
        "    test_df['pred'] = y_pred\n",
        "    return test_df[['date', permno_col, ret_col, 'pred']]\n",
        "\n",
        "\n",
        "# Parallelize processing across test years using Dask\n",
        "delayed_tasks = [delayed(process_year)(test_year, df, predictors, ret_col) for test_year in test_years]\n",
        "predictions = compute(*delayed_tasks)  # Compute all in parallel; auto-uses multi-cores\n",
        "\n",
        "all_pred = pd.concat(predictions)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# EVALUATION METRICS (Hint #9)\n",
        "# ============================================================\n",
        "def calculate_r2_oos(y_true, y_pred):\n",
        "    \"\"\"Calculate out-of-sample RÂ² as per Gu et al. (2020) Equation (19): 1 - SSE / total sum of squares (no mean subtraction).\"\"\"\n",
        "    sse = np.sum((y_true - y_pred) ** 2)\n",
        "    total_ss = np.sum(y_true ** 2)\n",
        "    return (1 - sse / total_ss) * 100  # As percentage\n",
        "\n",
        "\n",
        "y_true = all_pred[ret_col]\n",
        "y_pred = all_pred['pred']\n",
        "\n",
        "r2_oos = calculate_r2_oos(y_true, y_pred)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "medae = median_absolute_error(y_true, y_pred)\n",
        "\n",
        "print(\"Elastic Net Results:\")\n",
        "print(f\"RÂ²_oos (Gu et al. Eq. 19): {r2_oos:.2f}%\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
        "print(f\"Median Absolute Error: {medae:.4f}\")\n",
        "\n",
        "# Save predictions (Hint #12)\n",
        "all_pred.to_csv(data_dir + 'predictions.csv', index=False)\n",
        "print(\" Predictions saved to \" + data_dir + \"predictions.csv\")"
      ],
      "metadata": {
        "id": "BHAfql7gpW4l",
        "outputId": "ef8cd94c-c218-4753-a16b-737ff72e42db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 26) (ipython-input-2443794992.py, line 26)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2443794992.py\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    data_dir = '/content/Problem3/\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uHfi8g1IpdPt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}